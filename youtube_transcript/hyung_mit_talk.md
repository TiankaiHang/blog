## 不要教导，要激励 – 大型语言模型的未来

大家下午好！今天很荣幸请到来自 OpenAI 的研究员 [演讲者姓名] 。有趣的是，他虽然在麻省理工学院获得了博士学位，但研究方向却不是机器学习，而是可再生能源和能源系统。之后，他加入了 Google Brain，现在就职于 OpenAI。他在大型语言模型领域享有盛誉，之前的研究成果包括扩展 Flan、Flan-T5、Flan-PaLM 和用于训练 PaLM 模型的训练框架 T5X。今天，他将和我们分享大型语言模型领域的进展。让我们以热烈的掌声欢迎他！

### 引言

非常高兴回到这里！我今天演讲的主题是“不要教导，要激励”。我的研究方向是开发通用人工智能，而不是专门的人工智能。在开发通用人工智能的过程中，我们不可能枚举所有想教给模型的东西，因为我们期望模型具备的技能太多了，甚至连我们自己都不知道所有技能。因此，我认为实现通用人工智能唯一可行的方法是，对模型进行弱激励，让模型自行学习和发展。

在深入探讨细节之前，我先来谈谈今天演讲的目标，它与大多数技术演讲略有不同。我今天不会分享具体的技术论文或最新的实验结果，而是想以人工智能为例，分享我的思考方式。你可能会问，为什么我认为我们这些技术人员过于专注于解决问题本身？在我看来，我们应该花更多精力去寻找更有价值的问题来解决。我职业生涯中见过的最优秀的研究人员，不一定是最强的技术人员，当然他们也具备很强的技术能力，但这并不是他们的决定性特征。他们更擅长发现最有影响力的问题，而这源于他们拥有独特的视角和思考方式。我认为，拥有良好的视角非常重要，但这一点却被低估了。我希望借此机会分享我的视角，我并不是说我的视角有多么高明，而是希望通过分享，能够激发你们中的一些人对这类话题的兴趣，并促使我们整个社区更多地讨论这个问题，从而更好地找到更有价值的问题。

### 演讲提纲

今天演讲的提纲如下：

1.  **我的视角：**  我将分享我的视角，我所说的一切都将围绕着“扩展”展开，稍后我将定义我所理解的“扩展”。
2.  **从通用人工智能到语言模型：**  我们将以通用的 AI 研究为基础，逐步探讨我目前研究的语言模型。

### 计算能力的指数级增长

让我们从人工智能领域最重要的数据点开始说起。这张图来自 Rich Sutton 去年在主题演讲中的分享。横轴是时间，从 1900 年到 2020 年；纵轴是计算能力，即花费 1000 美元所能获得的每秒计算次数，这是一个对数刻度。我们可以看到，在过去的一百多年里，特定成本下的计算能力呈指数级增长，换句话说，计算成本呈指数级下降。我不知道还有哪个趋势像这个趋势一样强劲和持久。每当我看到这种超出我直觉的惊人趋势时，我都会想到两件事：第一，我不应该与之竞争；第二，我要尽可能地在职业生涯和生活的各个方面利用它。

我们看到硬件呈指数级增长，因此我们作为软件和算法开发人员，应该跟上这个趋势。特别是，我们需要更具扩展性的方法，以便更好地利用日益丰富的计算资源。

### 人工智能研究者的工作

更概括地说，人工智能研究者的工作是教会机器如何思考。一种非常普遍但不幸的做法是，我们试图教会机器像我们一样思考。但问题是，我们真的了解我们是如何思考的吗？答案是否定的。因此，在这种方法中，我们实际上是在教授我们自己都不完全理解的东西，而且是用有限的数学语言来教授。这种方法通常会对问题强加一种结构，而这种结构在进一步扩展时，通常会成为瓶颈。

Rich Sutton 的另一个重要观点是，他将人工智能在过去 70 年的进展总结为：开发更通用的方法，减少人为结构，并添加更多的数据和计算能力，换句话说，就是“扩展”。这是一个非常有力的论断，因为我们已经看到了许多不同类型的进展，但所有这些进展都可以归纳到这个非常有力的论断中。我完全同意这一点，事实上，我认为这是人工智能领域最重要的文章之一，我多次回顾这篇文章，如果你还没有读过，我强烈推荐你去读一读。

### 更少的结构，更多的扩展

这是我对同一观点的图形化解释。横轴是计算量，你可以将其理解为计算能力或数据量；纵轴是性能，你可以将其理解为某种智能指标。我们可以想到两种方法：一种是结构较多的方法，另一种是结构较少的方法。我们反复看到的是，结构较多的方法通常启动速度更快，因为结构本身充当了一种捷径，这种优势会一直持续到它失效为止，而且它不能真正地扩展。相比之下，结构较少的方法通常一开始表现不佳，因为我们给了模型太多的自由度，它不知道如何利用这些自由度，直到我们提供足够的数据、计算能力和良好的算法。在某个时刻，它会变得越来越好，我们称之为更具扩展性的解决方案。

举一个具体的例子，比如经典的机器学习算法，如支持向量机（SVM），与深度学习相比，它更像是一种结构化的版本，特别是像核方法这类方法，其结构旨在学习我们应该在数据中加入什么样的表示，而深度学习则是让模型在给定问题的情况下，自行学习数据的表示。这种方法一开始效果不佳，但最终却取得了成功，因为它更具扩展性。在深度学习领域，我们也看到了很多这样的例子，一些深度学习方法比其他方法更具扩展性。

### 对人工智能发展的一些思考

让我们来做一些发人深省的观察：

*   **人为结构的局限性：** 人类研究人员设计的巧妙结构，在扩展时通常会成为瓶颈。
*   **长期利益与短期利益的冲突：** 从长远来看，好的东西在短期内几乎必然看起来很糟糕。因为我们正在做的是基于学习的方法，我们试图给机器尽可能多的自由，所以从长远来看，能够胜出的东西在短期内看起来会很糟糕。这与其他科学领域有着截然不同的 динамика。
*   **计算能力的快速发展：** 计算能力的提升速度比我们成为更优秀的研究人员的速度更快。因此，我们不应该与之竞争，而应该赋予机器更大的自由度，让它们自己选择如何学习。这可能不同于人类所认为的人类智能，但我认为人类智能并不是任何形式的上限，我们最终关心的是智能本身及其创造的价值。因此，我们不应该规定智能应该是什么样子。

这些观察结果对你来说可能很明显，但至少很多人觉得这并不明显。造成这种情况的原因有很多，我想谈谈其中一个很少被提及的原因：研究人员希望添加建模思想，因为这在学术上更令人满意。有些人认为仅仅扩大规模是不科学的，甚至不值得研究。我经常听到这样的话：“这只是无聊的工程”。我想问这些人一个问题：我们为什么要研究人工智能？我们为什么要发展任何技术？我认为，最终的目标是创造价值，造福人类，这比任何科学家的个人成就都重要。因此，我们应该专注于最大限度地发挥人工智能的价值，最大限度地减少其负面影响，而不用去管是哪个学科领域实现了这一目标。如果我研究了 10 年的东西，现在不是最具扩展性的方法，不能以最佳方式利用计算资源，那么我就应该重新思考和学习。我认为这就是我一直以来所采取的方法，包括我的研究课题一直在变化，但在更好地利用计算资源这一主题上，它从未改变。

### 对“扩展”的定义

我一直在用一种比较宽泛的方式谈论“扩展”，我想更精确地定义一下我所说的“扩展”，因为它与通常使用的定义略有不同。通常的定义是，用更多的机器做同样的事情。我认为这没有错，但这并不有趣。我所理解的“扩展”包括：

1.  **识别瓶颈：** 识别阻碍进一步扩展的建模假设或归纳偏差。
2.  **替换瓶颈：** 用更具扩展性的方法替换它。

如果我们回顾之前支持向量机和深度学习的例子，我认为这就是一种“扩展”，因为我们想投入更多的计算资源，而支持向量机恰恰是瓶颈所在。我们发现数据表示是瓶颈，并用学习表示来代替它，我认为这就是我所理解的“扩展”，我认为这是一种更有趣的“扩展”。有很多这样的例子，在我个人看来，OpenAI 相对于其他机构做得非常好的一点是，那里的研究人员确实遵循了这种理念，无论是有意识地还是无意识地，这确实是我们在做的事情。我们有这么多可用的计算资源，或者说很快就会有这么多可用的计算资源，问题是我们如何利用它们。如果你现在有 1000 亿美元的计算资源，你能正确地使用它们吗？我认为没有人真正知道如何做到这一点。因此，我们正在寻找一种方法来利用不断增长的计算资源，这就是研究人员的工作。

### 语言模型

这就是我的视角。我接下来要讲的一切都将基于这种“扩展”和“苦涩的教训”。现在让我们来谈谈语言模型，我把它写成“大型语言模型”（LLMs），因为它是一个常见的缩写，但我认为这两个“L”都没有什么意义。“大型”是一个非常主观的词，今天的“大型”模型在几年后，甚至明年就会变成“小型”模型。另一个“L”是“语言”，我将谈谈为什么我认为它不是一个合适的词。

到目前为止，所有的大型语言模型都使用 Transformer 架构，我们不必深入研究细节，我们将从功能的角度来理解它，将 Transformer 视为一个序列到序列的映射，其中包含许多模块。输入是一系列向量，每个向量的大小为 d，共有 n 个向量；输出也是一样，至少在训练过程中是这样。交互是通过许多模块完成的。

#### 下一个词预测

我在这里简单介绍一下下一个词预测的过程，并探讨该过程的一些含义。

我们从一个序列模型开始，可以想到的一个特定序列是句子，它是一个单词序列。第一步是将其标记化，为什么要这样做呢？因为我们需要在计算机中表示单词，这需要某种编码机制。在这里，我们通常使用整数映射，我们定义一个有限的整数集，比如在这个例子中是 30,000 个整数，每个单词或标记都被映射到一个整数，这个整数在训练过程中会发生变化。现在我们有了一个整数序列，而不是字符串序列。

接下来，我们可以进行嵌入，这不是必须的，但这是比较常见的做法。现在，序列中每个元素（在本例中是标记）的整数表示都被表示为一个大小为 d 的向量。

现在让我们从基本原理出发，思考我们希望从序列模型中得到什么。序列模型是一个对序列中元素之间的交互进行建模的模型。Transformer 是一种特殊的序列建模方法，它将这种交互建模为一系列点积运算。我们取两个元素的向量表示，计算它们的点积，如果点积很高，我们就认为这两个元素在语义上是相关的；如果点积很低，我们就认为它们不相关。我们并没有真正规定语义上的相关性意味着什么，我们可以尝试去理解它，但我认为这并不容易，也可能没有意义。

经过一系列点积运算后，我们得到了一个大小相同的向量表示，但现在每个向量都知道其他向量存在，并且希望以某种方式度量它们之间的相关性，并对正在发生的事情有更深入的理解。

最后一步是定义损失函数，我们需要一个标量，因为我们想使用基于梯度的优化方法。我们计算这个标量相对于模型中所有参数的梯度，然后使用某种梯度方法进行更新，这就是一步。重复这个步骤尽可能多次，这就是训练过程。

#### 下一个词预测的魔力

接下来，让我们来谈谈从向量表示到单个标量（即优化目标函数）的过程。在这个例子中，现在大多数大规模训练都使用某种形式的下一个词预测，所以我认为值得更详细地研究一下。

我在幻灯片中放回了原始句子。我们所做的本质上是，给定前面的所有标记，尝试预测下一个标记。正如我提到的，这种编码机制从一个有限的词汇表开始，比如包含 100,000 个词条。模型输出的是在这些词条上的概率分布，所以这里的数字加起来应该等于 1。我们希望模型能够学习到句子的结构，使得下一个标记（即单词）的概率高于其他词条，比如“智能”，“许多智能”是没有意义的。

之后，我们再次尝试预测下一个标记，给定前面的所有单词，以此类推，直到结束。这就是整个过程。

从条件概率的角度来思考，我们是在给定前面所有标记的情况下，尝试预测下一个标记。对于有统计学背景的人来说，另一种等价的、可能更自然的想法是，我们将这些条件概率相乘，就得到了整个序列的概率，这就是我们要最大化的目标。这就是最大似然估计框架，但我认为将每个预测视为一个任务，在给定前面所有内容的情况下预测下一个标记，这种多任务学习的思路更直观。至关重要的是，每个预测的权重相同，我们不会区分哪个标记是什么。在工业规模的应用中，我们只需将网络规模的文本数据输入到 Transformer 中，神奇的事情就会发生。

那么，我们会期待什么样的魔力呢？这是一篇来自 PaLM 论文的结果，是我们在 Google 工作时发表的。我之所以选择这篇论文，是因为现在人们不再报告只进行预训练的结果。因此，如果想了解下一个词预测本身的性能，我们必须回顾一下这篇旧论文。

这里的细节并不重要，总之是一些大型基准测试任务。我们展示的是，仅仅通过预测下一个标记，实际上就能在许多不同的任务上超越人类平均水平。我不知道对你来说这意味着什么，但对我来说，仅仅通过预测下一个词就能在这么多不同的任务上做得这么好，这简直就是魔法。我仍然觉得这非常令人惊讶，我认为我们不应该认为这是理所当然的。

#### 对下一个词预测的观察

让我们对下一个词预测做一些观察：

*   **语言的涌现：** 我们没有直接教授任何语言学概念，比如下一个标记是动词，下一个标记是什么，我们没有做任何这样的事情。但不知何故，仅仅通过预测下一个标记，语言就涌现出来了。我认为这几乎是执行此类任务的副产品。我甚至可以进一步说，教授语言的最佳方式是不要教授语言，因为我们不想在机器学习语言的过程中强加任何结构。因此，这是一个非常有趣的副产品，这就是为什么我认为语言模型中的“L”是一个误称，因为我们没有教授任何语言。语言模型是关于学习一些词语知识，而语言只是其中一种表现形式。
*   **推理、数学和编码能力的涌现：** 另一个有趣的事情是，模型可以进行推理、数学和编码，即使我们没有进行任何专门的教学。

我认为所有我认识的研究人员，基本上都对为什么这种方法如此有效，提出了一些假设。但我认为，对于为什么这种方法如此有效，目前还没有达成共识。我确实有我自己的版本，它对我的研究项目很有帮助，所以我想在这里分享一下。

#### 我的假设：大规模隐式多任务学习

我的版本是，这是一种大规模的隐式多任务学习。“隐式”的意思是，我们没有直接指定模型要执行哪些多任务，它们只是由我们使用的大型语料库隐式地施加的。

让我们来仔细想想这个问题。我认为现在我们已经理所当然地认为语言模型做得很好，但也许回顾一下模型在训练过程中经历的痛苦是很有用的。让我试着用一个例子来说明预测下一个标记是什么感觉。

我将展示一个句子，带下划线的词是我要预测的词。在预测的过程中，让我们尽可能地减少工作量，尽可能地走捷径。

如果我想预测“这部糟糕的电影真的很 \_\_\_”，然后尝试预测“无聊”，我可能会走捷径，说“糟糕”是一个负面的词，所以它后面应该接一个负面的词，然后它是“电影”，所以也许我可以使用这种模式匹配来得到“无聊”这个词。

现在我再给你一个句子：“财报电话会议结束后，谷歌的股价上涨了 5%，收于 1050 美元”。因为我想尽可能地减少工作量，所以我就在想，我能不能重复使用从第一个例子中学到的技巧来完成这个任务？答案是不能，因为这是不相关的，我必须学习某种新的技能，那就是数学。

第三个例子是用完全不同的语言写的，所以我不能重复使用之前的技巧，因为我需要学习新的语言。这是我学到的第三个技能。

第四个例子：“热力学第一定律……”，这是我最喜欢的课题，曾经是，它被称为能量守恒定律。现在，为了预测这一点，我们需要知道，我们不能重复使用上面的任何东西，必须学习一项新的技能，那就是科学知识，等等，等等。

当我们在大规模的网络文本数据上进行训练时，我们谈论的是数十亿甚至数万亿个句子。由于每个句子包含多个下一个标记，数量级可能达到数千，因此我们谈论的是数万亿种可以用这种方式构建的任务类型。如果你试图解决这个问题，即使对于大型模型来说，在某些时候这也是难以处理的。

我认为，如果我们真的想以尽可能少的努力来完成这项任务，也许更不可避免的做法是，思考我可以学习哪些通用技能，并将这些技能应用到尽可能多的任务类型中，这可能是更容易的途径。

如果我们用数万亿种任务类型来训练模型，我认为这就是我对多任务学习假设的一种非常简单的理解。超过一定规模后，这样做会更容易。

我所说的“通用技能”是指：

*   **理解语言：** 首先，这些是文本数据，所以理解语言可能是我想学习的东西，即使我们没有教授任何关于语言的知识。我们可能会在模型中输入数百种语言，所以学习所有这些语言，可能至少对理解语言的语法结构非常有用。
*   **理解语义：** 其次，我想理解语言的语义，这样我就可以将其应用于几乎所有句子的下一个词预测。
*   **推理能力：** 第三，可能更像是推理能力，以便组合我所理解的概念。我只是在举例说明，但我认为这些技能就像，如果我正在执行这项任务，我可能会想到的技能。在某种程度上，这就是施加给模型的压力。

至关重要的是，我们没有直接教授任何这些通用技能，它们只是由学习目标和数据激励而产生的。而涌现出来的能力通常更通用，可能是因为它是由发展通用技能的需求驱动的。

#### 教导与激励

你可能会想，这种弱激励模型的方法需要更多的计算资源，因此效率低下。我想这样说：它使得添加更多计算资源成为可能，而且我们将会拥有更多计算资源。这是一种更具扩展性的训练策略，我认为这就是为什么这种方法取得如此成功的原因。

也许我们可以把这个假设推广一下：对于给定的数据集和学习目标，存在一个显式的学习信号和一组诱导的激励。

对于下一个词预测，显式的信号是预测下一个词，因为这是给定的任务。而诱导的激励则是学习通用技能，以便它能够处理尽可能多的任务类型，比如理解语言、推理等等。

#### 其他例子

*   **下棋：** 以使用0-1奖励机制（游戏结束时才给予奖励）的下棋游戏为例。如果你想想 AlphaGo Zero，它的显式信号是赢得游戏，除此之外我们没有教给它任何东西。因此，在这种情况下，诱导的激励是学习什么是好的走法。模型甚至可能不会考虑什么是“好”，以及“好”的概念，但为了赢得游戏，它必须知道哪些走法是好的。对于象棋来说，环境是如此狭窄，以至于这可能无关紧要。但如果我们使用这种方法，它所学到的东西，赢得游戏可能很重要，但在这种环境中什么是“好”的这种通用概念，才是真正重要的东西，我认为我们应该越来越关注这一点。
*   **幻觉问题：** 第三个例子可能与现在更相关，幻觉问题是语言模型尚未解决的核心问题之一。以下是一个我从 John Schulman 的演讲中得到并为我的目的改编的例子。假设我们正在做一个简单的问答任务，模型在回答简单问题时出现了幻觉。我们可以定义这样的奖励结构：如果答案正确且不模棱两可，则奖励为 1；如果答案正确但模型说“我不确定，但也许是这个”，则奖励为 0.5；如果模型说“我不知道”，这是一个重要的答案类别，则奖励为 0；如果答案模棱两可但错误，则奖励为 -2；如果模型非常自信地给出了错误答案，则奖励为 -4。在这种结构中，显式的信号是正确回答问题，但如果我们用数万亿个问题来训练模型，那么诱导的激励是什么呢？我认为是让模型知道它不知道什么。让我解释一下这意味着什么。
    
    如果你回答 100 个问题，你知道其中 50 个问题的答案，你可以记住哪些问题是你不知道答案的。但是，如果你要回答数万亿个问题，那么与其追踪每一个问题，知道哪些问题，不知道哪些问题，不如想想什么是“知道”、什么是“不知道”的通用概念。这种能力可能是从这种诱导的激励结构中涌现出来的，我认为这是解决幻觉问题的唯一根本途径。
    

#### 钓鱼的比喻

也许可以用一个不太恰当的比喻来解释这个过程。有一句古老的谚语说：“授人以鱼，不如授人以渔”。你可以把第一种理解为硬编码，第二种是直接教授技能。那么，对于这个问题，基于激励的方法是什么呢？是让他尝到鱼的味道，让他感到饥饿，然后如果他被激励了，他就会出去学习很多技能，包括钓鱼。在这个过程中，诱导的激励是，他会学习其他一些东西，比如耐心、了解天气、知道鱼喜欢什么诱饵等等。而这些可能是更通用的技能，比如耐心是一种非常通用的技能。

这就是我对这句古老谚语的不太恰当的扩展。

#### 时间与计算资源的权衡

看到这里，你可能会想，这效率太低了，为什么要这样做呢？在实现目标和花费时间之间存在一个权衡。更一般地说，当存在权衡时，我们考虑的是一种稀缺资源，在这种情况下，是完成这项工作所需的时间。

如果我们把它画在图上，横轴是每种方法完成工作所需的时间，那么激励他学习钓鱼技能所需的时间要长得多。但这种限制是针对人类的。对于机器来说，在考虑权衡时，我们谈论的稀缺资源不是时间，虽然也是时间，但越来越多的是所需的计算资源，我们可以克服这一点。

#### 专家与通才

这其中有一个非常重要的含义。我经常听到有人说，我们有一个小型的专家模型，它可能比一些大型的通才模型要好。当我听到这句话时，我发现这里隐藏着一个假设，那就是小型专家模型可以在狭窄的领域中胜出，因为在成为专家和通才之间存在某种权衡。我认为这是我们作为人类的偏见。

为什么呢？因为我们再次想到了权衡，我们想到了稀缺资源，那就是时间。大多数人在相当长的时间预算下运作，你可能会少睡一个小时或多睡一个小时，但大致上每天都有 16 到 17 个小时，这就是人类的时间预算。因此，没有花在成为通才上的时间，就是花在成为专家上的时间，反之亦然。

但对于机器来说，情况并非如此。一个模型可以比其他模型享有更多的计算资源。就像漫画《龙珠》中的精神时间屋，你在里面训练一年，外面才过了一天。

在这种情况下，能够进入精神时间屋的人拥有的时间比其他人多 365 倍。但当我们谈论机器时，这个倍数就不是 365 倍了，而是要高得多，我认为这就是现状。

所以我认为这种权衡根本不存在，如果说有什么区别的话，那就是通才实际上更擅长处理专业的事情。

#### 智能门槛

你可能会想，这种激励结构的想法并不新鲜。如果说有什么区别的话，那就是整个人类文明可能是由激励驱动的，甚至进化也可以被认为是一种为了生存而存在的激励结构。

那么，为什么我要谈论这个古老的想法呢？为什么它现在变得重要了呢？这就是我需要说明的。

让我从这句话开始：

> 无论多少香蕉，都不可能激励猴子进行数学推理。

即使奖励是无限的，激励结构是完美的，这也不可能实现。这意味着，对于给定的问题，需要某种程度的智能门槛，才能使激励结构发挥作用。而目前，像 GPT-4 这样的模型已经跨越了许多任务的智能门槛。

所以我认为，现在开始考虑使用基于激励的结构来教导模型我们关心的事情，肯定是合理的。

这就引出了“智能门槛”的概念。这种激励结构取决于模型的大小或模型的规模，涌现出来的能力取决于模型的大小，这是一个有趣的参数。如果模型太小，模型可能会放弃发展这些通用技能，而满足于简单的启发式方法，并且在训练过程中损失很高。

#### 涌现能力

我已经用一种比较宽泛的方式谈论了“涌现能力”，我想更详细地谈谈这个问题。

特别是，我们谈论的是，当我们扩大模型规模时，一些能力会涌现出来。这需要我们有正确的视角。

在这个领域，我们有一些讨论是关于涌现能力是否是海市蜃楼，或者涌现能力是否会发生等等。我认为细节并不重要，让我来告诉你一种思考方式。

我制作了一个最小的 Transformer 模型，模型大小为 1，只有两层，一个非常小的 Transformer 模型。然后我选择一个 GPT-4 可以完成的任务，比如数学，然后用这个最小的模型来解决它，它肯定无法完成。而 GPT-4 可以完成，所以在这两者之间存在某种拐点。

对于任何问题，你都可以找到一个最小的模型，它无法完成这个任务。如果更大的模型可以完成，那么对我来说，这就是涌现。

所以我认为，很容易证明涌现能力确实存在。如果它不存在，那么对我来说，它可能就不够有趣了。

这就是我的意思。横轴是某种规模，一开始它不能完成任务，然后在某个时候，它突然就涌现出来了。并不是所有任务都是这样，但在许多有趣的任务中，比如推理，我们一遍又一遍地看到这种模式。

#### 新的视角

我们需要思考的第一个视角是：**“还没”的视角。** 现在不能做的事情，并不意味着以后也不能做。所以我们应该做的是，不要说这个想法行不通，而应该说这个想法**现在还**行不通。

对于人类来说，这种想法是很不自然的。我认为这是因为我们所处的环境，其基本公理变化不大，就像物理学一样。如果你现在做一个热力学实验，你得到一个结果，三年后你再做同样的实验，结果可能还是一样，即使 300 年后，结果也可能还是一样。我们习惯了这种环境，因为我们所处的环境的基本公理是物理学。

那么，对于语言模型来说，与我们所处的环境的基本公理相对应的概念是什么呢？我认为是**最强大的模型**。

2020 年，GPT-3 问世，它成为了研究人员构建如何利用这类技术的直觉的公理。去年，GPT-4 问世，它改变了很多东西，许多建立在旧模型之上的东西都变得不再适用。这几乎就像每隔几年，我们就搬到了一个不同的星球，我们对物理世界的理解也发生了变化。

这就要求我们不断地进行**反学习**。建立在那些不正确的公理之上的直觉和想法，现在必须被摒弃。但我认为这种情况并不常见。

#### 新人的优势

这对领域新人来说是一个有趣的现象。我经常看到，没有多少研究经验的本科生，进入这个领域一年后，就能写出改变整个领域发展轨迹的论文。如果你想想理论数学或理论物理学，我认为这是不可能的，除非你回到高斯时代。

所以这是一个非常有趣的现象。如果你没有太多知识储备，你可能会认为自己没有太多包袱需要丢弃，你更灵活，至少我是这样想的。OpenAI 的很多人不是传统的机器学习博士，他们更像是，当他们看到证据发生变化时，他们总是会适应新的范式。

我认为这是一个非常重要的现象，我想指出来。

#### 涌现能力的曲线

我再用最后一个图表来说明一下我对涌现能力的看法，然后我们就可以总结了。

横轴是某种规模，纵轴是能力。我画了两个不同大小的模型，GPT-3 和 GPT-4，但这并不重要。

让我们先来看看能力 1。对于能力 1，GPT-3 和 GPT-4 都还没有达到拐点，所以它们都无法完成这项任务。但一个比 GPT-4 稍微好一点的模型，可能就能看到一些成功的迹象。

能力 2 需要更长的时间才能涌现出来。

对于能力 3，即使是 GPT-3 也能做到，比如情感分析这类任务。

我想说的是，当我们看到一种能力涌现出来时，由于我们没有任何关于涌现能力的理论，所以我们无法知道我们正在研究的是哪种能力。但至少拥有这种视觉化的图表，可以让我做好准备，作为一个实用主义者，我不想研究能力 2，我想研究能力 1，为下一个模型做好准备，思考为什么这个模型现在还不能完成这项任务，我可以在哪里投入更多计算资源，让它看到一些成功的迹象。

也许研究那些小型模型已经能够完成的任务并没有什么错，但我只是想说，如果我正在研究那些小型模型已经能够完成的任务，我需要知道，它的改进空间是有限的。

### 总结

让我用几句话来总结一下我们今天讨论的内容：

*   **计算成本呈指数级下降**是最重要的潜在驱动力。
*   **人工智能研究者的工作**是认识到这一点，不要与之竞争，而是通过设计更具扩展性的方法来利用它。
*   **当前一代语言模型**依赖于下一个词预测，这可以被认为是一种弱激励结构，它需要学习通用技能，以便能够处理数万亿种任务类型。
*   **更概括地说**，我们应该开始思考，下一个词预测已经很棒了，但它不是唯一的选择，我们应该认真思考模型的激励结构，这是我们应该积极参与的一种新的学习范式。
*   **涌现能力**是语言模型的独特之处之一，我们需要有一些正确的视角，比如摒弃那些基于过时公理的想法。

谢谢大家！
